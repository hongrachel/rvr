# Representations via Representations
Zhun Deng, Frances Ding, Cynthia Dwork, Rachel Hong, Giovanni Parmigiani, Prasad Patil, and Pragya Sur

Representation via Representations is a project aimed at improving transfer learning to out-of-distribution examples. Motivated by the challenge of finding robust biomedical predictors of disease, the model leverages data from heterogenous sources to discover feature representations that allow for accurate prediction outside of the training data.

This codebase owes its foundations to David Madras, Elliot Creager, Toni Pitassi, Richard Zemel in their paper Learning Adversarially Fair and Transferable Representations (https://arxiv.org/abs/1802.06309). Github link: https://github.com/VectorInstitute/laftr

## setting up a project-specific virtual env
```
mkdir ~/venv 
python3 -m venv ~/venv/rvr
```
where `python3` points to python 3.6.X. Then
```
source ~/venv/rvr/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```
or 
```
pip install -r requirements-gpu.txt
```
for GPU support

## running a single fair classification experiment
```
source simple_example.sh
```
The bash script first trains LAFTR and then evaluates by training a naive classifier on the LAFTR representations (encoder outputs).
See the paper for further details.

## running a sweep of fair classification with various hyperparameter values
```
python src/generate_sweep.py sweeps/small_sweep_adult/sweep.json
source sweeps/small_sweep_adult/command.sh
```
The above script is a small sweep which only trains for a few epochs. 
It is basically just for making sure everything runs smoothly. 
For a bigger sweep call `src/generate_sweep` with `sweeps/full_sweep_adult/sweep.json`, or design your own sweep config.

## workflow for experiments
1. If creating a new dataset, save the data in an `.npz` file (eg. `mnist_sample.npz`) with the data as numpy vectors in the following fields: “x_train”, “x_test”, “y_train”, “y_test”, “attr_train”, “attr_test”, “train_inds”, “valid_inds”, and store the file in the `data/` folder under either an existing folder or a new name. Additionally include other fields if there is other important information.
2. Add a line in `utils.py` to point the basename to the correct file name (`.npz` file)
3. Create a new `.json` file in the conf/templates/data/ and conf/templates/model/ folders, using the same name as the data folder name.
4. Create a new folder in `sweeps/` (can essentially copy another one, especially the sweeps/[NAME]/config.json file which stays the same if defaults are the same) 
5. In the sweep.json file, make sure to change sweep_name, config, and data to the right names for the new experiment
6. Decide what parameters I want to run for the new experiment
7. Run python/src/generate_sweep.py on the sweep.json file to get the commands file
8. Divide up the commands file into separate files with create_slurmfiles.sh (if needed)
9. Change the .sbatch files to have the right parameters and point to the right bash scripts
10. Submit the job to SLURM. For more details visit https://docs.rc.fas.harvard.edu/kb/running-jobs/

## data
The synthetic datasets are provided in `data/runhet/*.npz` or `data/runorfunc/*.npz` Due to space constraint, all the dataset generation files for synthetic data and colored MNIST can be found in `src/data_processing/`. The dataset generation code for PACS is pulled from https://github.com/ameroyer/TFDatasets.

The biomedical data is pulled from the curatedOvarianData through Bioconductor, with documentation found at http://www.bioconductor.org/packages/2.12/data/experiment/html/curatedOvarianData.html. The relevant `.csv` files need to be downloaded online due to size constraints, which can be found in section 10 of http://bioconductor.org/packages/release/data/experiment/vignettes/curatedOvarianData/inst/doc/curatedOvarianData_vignette.pdf. Note that R is needed in order to access the package. The correctly formatted `.npz` files for the model are generated by running `data/biomedical_data/create_dataset.ipynb`.

## model
The implementations of the models (encoder, classifier, auditor) can be found in src/codebase/models.py The network architecture itself is found in `src/codebase/mlp.py` with both feed-forward and convolutional neural network options. See the paper for exact details

## configurations
`templates/data.json`: contains .json files giving a name and some metadata about each dataset. The only attributes really used are “name”, “use_attr”, and “multi”, where “use_attr” is a flag for whether to include the source identity in the input vector to the encoder. “Multi” indicates how many sources there are. The files relevant for digit recognition are mnist and mnist_irm
`templates/dirs.json`: tells the code where to store the output of the experiments; you will need to create a new file pointing to your directories
`templates/model.json`: contains json files that describe the dimensions / layer sizes for the model you want to use (a different file is used for each dataset, generally). “Enc” means encoder, “cla” is classifier, “rec” is reconstruction (decoder), “aud” is auditor (adversary). 

