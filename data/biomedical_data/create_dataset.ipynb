{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E.MTAB.386_eset_clindata.csv', 'E.MTAB.386_eset_exprs.csv', 'GSE12418_eset_clindata.csv', 'GSE12418_eset_exprs.csv', 'GSE12470_eset_clindata.csv', 'GSE12470_eset_exprs.csv', 'GSE13876_eset_clindata.csv', 'GSE13876_eset_exprs.csv', 'GSE14764_eset_clindata.csv', 'GSE14764_eset_exprs.csv', 'GSE17260_eset_clindata.csv', 'GSE17260_eset_exprs.csv', 'GSE18520_eset_clindata.csv', 'GSE18520_eset_exprs.csv', 'GSE19829.GPL570_eset_clindata.csv', 'GSE19829.GPL570_eset_exprs.csv', 'GSE19829.GPL8300_eset_clindata.csv', 'GSE19829.GPL8300_eset_exprs.csv', 'GSE20565_eset_clindata.csv', 'GSE20565_eset_exprs.csv', 'GSE2109_eset_clindata.csv', 'GSE2109_eset_exprs.csv', 'GSE26193_eset_clindata.csv', 'GSE26193_eset_exprs.csv', 'GSE26712_eset_clindata.csv', 'GSE26712_eset_exprs.csv', 'GSE30161_eset_clindata.csv', 'GSE30161_eset_exprs.csv', 'GSE32062.GPL6480_eset_clindata.csv', 'GSE32062.GPL6480_eset_exprs.csv', 'GSE32063_eset_clindata.csv', 'GSE32063_eset_exprs.csv', 'GSE44104_eset_clindata.csv', 'GSE44104_eset_exprs.csv', 'GSE49997_eset_clindata.csv', 'GSE49997_eset_exprs.csv', 'GSE51088_eset_clindata.csv', 'GSE51088_eset_exprs.csv', 'GSE6008_eset_clindata.csv', 'GSE6008_eset_exprs.csv', 'GSE6822_eset_clindata.csv', 'GSE6822_eset_exprs.csv', 'GSE8842_eset_clindata.csv', 'GSE8842_eset_exprs.csv', 'GSE9891_eset_clindata.csv', 'GSE9891_eset_exprs.csv', 'PMID17290060_eset_clindata.csv', 'PMID17290060_eset_exprs.csv', 'TCGA.RNASeqV2_eset_clindata.csv', 'TCGA.RNASeqV2_eset_exprs.csv', 'TCGA_eset_clindata.csv', 'TCGA_eset_exprs.csv']\n"
     ]
    }
   ],
   "source": [
    "folder = \"ovarian_data/\"\n",
    "files = sorted(os.listdir(folder))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_files = ['E.MTAB.386_eset_clindata.csv','E.MTAB.386_eset_exprs.csv','GSE13876_eset_clindata.csv',\n",
    "               'GSE13876_eset_exprs.csv','GSE17260_eset_clindata.csv','GSE17260_eset_exprs.csv',\n",
    "               'GSE26712_eset_clindata.csv','GSE26712_eset_exprs.csv', 'GSE9891_eset_clindata.csv',\n",
    "               'GSE9891_eset_exprs.csv', 'TCGA_eset_clindata.csv', 'TCGA_eset_exprs.csv',\n",
    "               'GSE18520_eset_clindata.csv','GSE18520_eset_exprs.csv','GSE19829.GPL8300_eset_clindata.csv',\n",
    "               'GSE19829.GPL8300_eset_exprs.csv','PMID17290060_eset_clindata.csv','PMID17290060_eset_exprs.csv']\n",
    "len(valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS TAKES A WHILE TO RUN\n",
    "# turn csv files into array to store in dictionary\n",
    "\n",
    "expr_datasets = {} # file name -> 2d array row = patient, col = gene\n",
    "pheno_datasets = {} # file name -> 2d array row = patient, col = phenotype\n",
    "\n",
    "for file in files:\n",
    "    if file in valid_files:\n",
    "        dataset = []\n",
    "\n",
    "        with open(folder + file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "            for row in reader:\n",
    "                dataset.append([row[0].upper()] + row[1:])  # turn gene names to upper case\n",
    "\n",
    "        if '_eset_exprs.csv' in file:\n",
    "            dataset = np.transpose(dataset)  # need to flip since csv files have row = gene, col = patient\n",
    "            sorted_dataset = dataset[:,np.argsort(dataset[0])]\n",
    "            expr_datasets[file.replace('_eset_exprs.csv', '')] = sorted_dataset\n",
    "        elif '_eset_clindata.csv' in file:\n",
    "            pheno_datasets[file.replace('_eset_clindata.csv', '')] = np.asarray(dataset)\n",
    "        else:\n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(list(expr_datasets.keys())))\n",
    "print(len(list(pheno_datasets.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 10358)\n",
      "(130, 32)\n"
     ]
    }
   ],
   "source": [
    "for key in expr_datasets:\n",
    "    print(expr_datasets[key].shape)\n",
    "    print(pheno_datasets[key].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_features(num_studies, expr_datasets):\n",
    "    intersected = []\n",
    "    count = 0\n",
    "    for i, key in enumerate(list(expr_datasets.keys())):\n",
    "        if i >= 0: # not in [1,2,9,10,16,19,20]:\n",
    "            genes = expr_datasets[key][0][1:] # trim first empty string\n",
    "            if len(intersected) == 0:\n",
    "                intersected = genes\n",
    "            else:\n",
    "                intersected = np.intersect1d(intersected, genes)\n",
    "\n",
    "            count += 1\n",
    "            if num_studies and count >= num_studies:\n",
    "                return (intersected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6063"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_studies = 9 # 15 training + 1 test group of 4 last studies\n",
    "features = intersect_features(num_studies, expr_datasets)\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130,)\n",
      "(130, 10358)\n",
      "(130, 32)\n"
     ]
    }
   ],
   "source": [
    "clean_expr_datasets = {} # file name -> 2d array row = patient, col = gene\n",
    "clean_pheno_datasets = {} # file name -> 2d array row = patient, col = phenotype\n",
    "count = 0\n",
    "for key in pheno_datasets:\n",
    "    dataset = pheno_datasets[key]\n",
    "    index = np.argwhere(dataset[0] == \"summarystage\")[0][0]\n",
    "    valid_patient_indices = (dataset[:,index] != \"NA\")\n",
    "    print(valid_patient_indices.shape)\n",
    "    clean_expr_datasets[key] = expr_datasets[key][valid_patient_indices]\n",
    "    print(expr_datasets[key][valid_patient_indices].shape)\n",
    "    clean_pheno_datasets[key] = dataset[valid_patient_indices]\n",
    "    print(dataset[valid_patient_indices].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 6064)\n",
      "(130,)\n",
      "(130, 6064)\n",
      "(130, 32)\n"
     ]
    }
   ],
   "source": [
    "clean_expr_datasets = {} # file name -> 2d array row = patient, col = gene\n",
    "count = 0\n",
    "for key in expr_datasets:\n",
    "    dataset = expr_datasets[key]\n",
    "    feature_col_mask = np.isin(dataset[0],features)\n",
    "    feature_col_mask[0] = True  # include patient names\n",
    "    valid_patient_mask = np.all(dataset[:,feature_col_mask] != \"NA\", axis=1) # make sure gene values exist here\n",
    "    print(dataset[:,feature_col_mask].shape)\n",
    "    print(valid_patient_mask.shape)\n",
    "    print(dataset[:,feature_col_mask][valid_patient_mask].shape)\n",
    "    print(pheno_datasets[key][valid_patient_indices].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out intersected features for patients w valid feature values (feature name in first value of each row)\n",
    "def filter_datasets_by_features(num_studies, eds, pds):\n",
    "    clean_expr_datasets = {} # file name -> 2d array row = patient, col = gene\n",
    "    clean_pheno_datasets = {} # file name -> 2d array row = patient, col = phenotype\n",
    "    count = 0\n",
    "    for i, key in enumerate(list(eds.keys())):\n",
    "        if i >= 0: # not in [1,2,9,10,16,19,20]:\n",
    "            dataset = eds[key]\n",
    "            feature_col_mask = np.isin(dataset[0],features)\n",
    "            feature_col_mask[0] = True  # include patient names\n",
    "            valid_patient_mask = np.all(dataset[:,feature_col_mask] != \"NA\", axis=1) # make sure gene values exist here\n",
    "            clean_expr_datasets[key] = dataset[:,feature_col_mask][valid_patient_mask]\n",
    "            clean_pheno_datasets[key] = pds[key][valid_patient_mask]\n",
    "\n",
    "            count += 1\n",
    "            if count >= num_studies:\n",
    "                return clean_expr_datasets, clean_pheno_datasets\n",
    "\n",
    "# get rid of patients in eds and pds with null pheno_name\n",
    "def filter_datasets_by_phenotype(num_studies, pheno_name, eds, pds):\n",
    "    clean_expr_datasets = {} # file name -> 2d array row = patient, col = gene\n",
    "    clean_pheno_datasets = {} # file name -> 2d array row = patient, col = phenotype\n",
    "    count = 0\n",
    "    \n",
    "    for key in pds:\n",
    "        dataset = pds[key]\n",
    "        index = np.argwhere(dataset[0] == pheno_name)[0][0]\n",
    "        valid_patient_indices = (dataset[:,index] != \"NA\")\n",
    "        clean_expr_datasets[key] = eds[key][valid_patient_indices]\n",
    "        clean_pheno_datasets[key] = dataset[valid_patient_indices]\n",
    "        \n",
    "        count += 1\n",
    "        if count >= num_studies:\n",
    "            return clean_expr_datasets, clean_pheno_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features_eds, filtered_features_pds = filter_datasets_by_features(num_studies, expr_datasets, pheno_datasets)\n",
    "assert (len(filtered_features_eds) == num_studies)\n",
    "for key in filtered_features_eds:\n",
    "    assert (len(filtered_features_eds[key][0]) == len(features) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 32)\n"
     ]
    }
   ],
   "source": [
    "for key in filtered_features_pds:\n",
    "    print(filtered_features_pds[key].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.MTAB.386\n",
      "[False False False  True  True  True  True  True False False  True False\n",
      "  True  True  True False  True  True False  True False False  True False\n",
      "  True  True  True  True  True False  True  True  True  True  True False\n",
      " False False  True  True  True  True False  True False  True  True False\n",
      "  True False False False False False  True  True  True  True  True False\n",
      " False  True  True  True False  True  True False False False False  True\n",
      " False  True False  True  True  True False  True  True  True False  True\n",
      " False False False  True False False False False  True  True  True False\n",
      " False False False False False False False  True  True  True False False\n",
      "  True  True  True  True  True  True False  True  True False False  True\n",
      "  True  True  True  True  True False  True  True False]\n",
      "GSE13876\n",
      "[ True  True False  True  True  True  True  True False  True  True  True\n",
      "  True False  True False  True  True  True False False  True  True  True\n",
      "  True  True  True False False False  True  True  True  True  True  True\n",
      " False  True  True  True  True False False  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True False  True False False\n",
      " False  True  True False  True  True False  True False  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True False  True\n",
      "  True False  True  True False  True  True  True False  True  True  True\n",
      " False  True False  True False  True  True  True False  True  True  True\n",
      "  True False  True False  True  True  True False  True  True False  True\n",
      " False  True  True  True  True False  True  True  True False  True False\n",
      "  True  True  True  True  True  True False  True  True False False  True\n",
      "  True  True False  True False False  True  True  True  True False  True\n",
      "  True]\n",
      "GSE17260\n",
      "[False  True False False  True False  True False False  True False False\n",
      "  True  True  True False False False  True  True  True  True  True  True\n",
      " False False False  True False False False  True False False False False\n",
      " False False False False False False False False False False False  True\n",
      "  True  True  True  True  True False  True False False False False  True\n",
      " False False  True  True  True  True  True  True  True  True False False\n",
      "  True  True False False  True False False  True False False False False\n",
      " False  True False False  True False False False False False  True  True\n",
      "  True  True  True False False False  True  True  True  True False False\n",
      " False False]\n",
      "GSE18520\n",
      "[False  True False False  True False  True  True False  True  True  True\n",
      " False  True  True  True  True  True  True  True False False  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True False  True  True  True  True  True\n",
      "  True  True  True  True False]\n",
      "GSE19829.GPL8300\n",
      "[ True False False  True  True False  True  True  True  True  True False\n",
      "  True False  True False False  True False  True False  True False False\n",
      " False False  True  True  True  True False  True False  True False  True\n",
      " False False  True  True  True False]\n",
      "GSE26712\n",
      "[False False False  True False  True False  True False False False  True\n",
      " False  True False False False False  True  True  True  True False  True\n",
      " False  True False  True False  True  True False False False False False\n",
      "  True False False  True False  True False False  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True False  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True False  True  True\n",
      "  True  True  True  True  True False  True False  True  True  True  True\n",
      "  True  True  True False False False  True  True  True  True False  True\n",
      "  True  True  True  True  True  True False  True  True  True  True False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True False  True  True  True  True False\n",
      "  True False  True  True False False False  True False  True  True False\n",
      " False False False False  True]\n",
      "GSE9891\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False  True False False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False  True False False False False\n",
      " False False False False  True  True False  True  True  True False False\n",
      "  True False False False False False False  True False False False  True\n",
      " False False False False False False False False  True False  True False\n",
      " False  True  True False  True  True  True  True False False False False\n",
      "  True False False  True  True  True  True  True  True False  True False\n",
      " False False False  True False  True False False False False  True  True\n",
      " False  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True False  True  True False  True False False  True  True\n",
      "  True False  True  True  True False  True  True False False False False\n",
      "  True False False  True False False False  True False False  True False\n",
      " False False False  True  True False  True False False False False False\n",
      " False False False False False False  True  True  True  True  True False\n",
      " False False  True False False  True False False False False  True  True\n",
      "  True False False False  True False  True False  True False  True False\n",
      "  True False False False False False False  True False  True  True False\n",
      " False  True False False False False False False False False  True False\n",
      " False False False False False False  True False False False False  True\n",
      "  True  True False False False False False  True False False  True  True\n",
      "  True False  True False False False False False False False  True False\n",
      "  True  True False  True False  True]\n",
      "PMID17290060\n",
      "[False False  True  True  True  True False  True False False  True  True\n",
      "  True  True  True  True  True  True  True False False  True  True  True\n",
      " False  True  True  True  True  True  True  True False False  True  True\n",
      "  True  True  True False False False  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True False  True  True  True\n",
      "  True  True False  True False  True False False  True  True False False\n",
      "  True False  True  True  True False False False False False False False\n",
      "  True  True  True False False  True False False  True  True False False\n",
      " False False False  True  True False False False False False False False\n",
      " False False  True  True False  True False False  True]\n",
      "TCGA\n",
      "[ True  True  True  True  True  True False  True False False False  True\n",
      "  True False False False False  True  True False False  True  True  True\n",
      "  True False  True  True False  True False  True  True False False  True\n",
      "  True  True  True False  True  True False  True False False False  True\n",
      "  True  True False  True  True False  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False  True  True False False  True  True False False  True  True\n",
      " False  True  True  True False False  True  True False  True  True  True\n",
      " False False  True  True False False False False  True  True  True False\n",
      "  True False  True  True  True  True False  True  True  True  True False\n",
      " False  True  True  True False False False  True  True False  True False\n",
      " False  True  True False  True False  True  True False False False False\n",
      "  True  True  True False False False False False False  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True False False  True  True False  True\n",
      " False False  True  True False  True False  True  True False False False\n",
      "  True  True  True False False  True  True False False  True  True False\n",
      "  True False False False  True  True False False False  True  True False\n",
      " False False  True  True False  True  True False  True False  True False\n",
      " False False  True False False False  True  True  True False False  True\n",
      "  True False False  True  True  True False False  True  True  True  True\n",
      " False  True False  True  True  True False False  True  True  True  True\n",
      " False  True  True  True  True False  True  True  True  True False False\n",
      "  True  True  True  True False False  True  True  True  True False False\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True False False  True  True  True False  True  True  True  True\n",
      "  True False  True  True False  True False  True False False  True  True\n",
      " False  True  True  True False  True False  True False False  True False\n",
      "  True False False  True False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False  True  True  True False False False False\n",
      " False False False  True  True  True  True False  True  True  True False\n",
      " False False False  True  True False  True False  True  True  True False\n",
      " False False  True  True False False False False  True False False  True\n",
      " False False  True  True  True False False  True False False  True False\n",
      " False  True  True False  True  True  True False False  True  True  True\n",
      "  True False False False  True False  True False False False False False\n",
      " False False False False  True False False False False False  True  True\n",
      " False False  True False  True False False False False False  True False\n",
      " False False False  True False False False  True  True False False False\n",
      "  True False False  True  True False False  True  True  True False False\n",
      "  True  True False False False  True False False False False  True False\n",
      " False  True  True False False False  True False False False False False\n",
      "  True  True  True False False  True  True  True False False False  True\n",
      "  True  True False False  True  True False False False False  True  True\n",
      "  True  True False False  True  True False False False False  True  True\n",
      " False False  True  True  True  True False False]\n",
      "0.5474006116207951\n"
     ]
    }
   ],
   "source": [
    "# find a good phenotype\n",
    "\n",
    "pheno_try = 'vital_status'\n",
    "pheno_pos = 'deceased'\n",
    "num_pos = 0\n",
    "num_total = 0\n",
    "for key in filtered_features_pds:\n",
    "    print(key)\n",
    "    dataset = filtered_features_pds[key]\n",
    "    index = np.argwhere(dataset[0] == pheno_try)[0][0]\n",
    "    valid_patient_indices = (dataset[:,index] != \"NA\")\n",
    "    labels = dataset[valid_patient_indices][:,index][1:] == pheno_pos # .astype(float) > pheno_pos\n",
    "    num_total += len(labels)\n",
    "    num_pos += np.count_nonzero(labels)\n",
    "    print(labels)\n",
    "print(num_pos * 1.0 / num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set phenotype here:\n",
    "pheno_name = 'vital_status'\n",
    "pheno_pos = 'deceased'\n",
    "\n",
    "clean_expr_datasets, clean_pheno_datasets = filter_datasets_by_phenotype(\n",
    "    num_studies, pheno_name, filtered_features_eds, filtered_features_pds\n",
    ")\n",
    "# assert (len(filtered_pheno_eds) == len(filtered_pheno_pds))\n",
    "# for key in filtered_pheno_eds:\n",
    "#     assert (len(filtered_pheno_eds[key]) == len(filtered_pheno_pds[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_datasets(n, pheno, eds, pds):\n",
    "#     clean_eds, clean_pds = filter_datasets_by_features(n, eds, pds)\n",
    "#     return filter_datasets_by_phenotype(n, pheno, clean_eds, clean_pds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set phenotype here:\n",
    "# pheno_name = 'summarystage'\n",
    "\n",
    "# clean_expr_datasets, clean_pheno_datasets = clean_datasets(num_studies, pheno_name, expr_datasets, pheno_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure dimensions are right\n",
    "\n",
    "assert (len(clean_expr_datasets) == num_studies)\n",
    "assert (len(clean_pheno_datasets) == num_studies)\n",
    "\n",
    "for key in clean_expr_datasets:\n",
    "    assert( len(clean_expr_datasets[key][0]) == len( features) + 1 ) # plus 1 for the patient identifiers\n",
    "    try:\n",
    "        assert ((clean_expr_datasets[key][0][1:] == list(features)).all())  # make sure order is correct\n",
    "    except AssertionError:\n",
    "        e = (clean_expr_datasets[key][0][1:])\n",
    "        for i in range(len(e)):\n",
    "            if e[i] != features[i]:\n",
    "                print (e[i])\n",
    "                print(features[i])\n",
    "        raise Exception\n",
    "    assert (len(clean_expr_datasets[key]) == len(clean_pheno_datasets[key])) # make sure same num patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_studies = 6  # rest are test studies\n",
    "test_studies = ['GSE18520','GSE19829.GPL8300','PMID17290060']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test GSE18520\n",
      "test GSE19829.GPL8300\n",
      "test PMID17290060\n",
      "1635\n",
      "1423\n",
      "212\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "# X, Y, A\n",
    "# each patient has gene expression x from study ID a with phenotype label y (let's do summarystage?)\n",
    "\n",
    "# X_test, Y_test, A_test (15th study = test?)\n",
    "\n",
    "# count stuff\n",
    "total_num = 0  # total number of samples\n",
    "train_num = 0\n",
    "test_num = 0\n",
    "count = 0\n",
    "train_study_nums = {} # key -> number of samples\n",
    "for key in clean_expr_datasets:\n",
    "    num_samples = len(clean_expr_datasets[key]) - 1 # ignore gene name column\n",
    "    assert (num_samples == len(clean_pheno_datasets[key]) - 1)\n",
    "    total_num += num_samples\n",
    "    \n",
    "    if key not in test_studies: # count < num_train_studies: #\n",
    "#         print(\"train\", key)\n",
    "        train_study_nums[key] = (num_samples)\n",
    "        train_num += num_samples\n",
    "    else:\n",
    "        print(\"test\", key)\n",
    "        test_num += num_samples\n",
    "    count += 1\n",
    "print(total_num)\n",
    "print(train_num)\n",
    "print(test_num)\n",
    "print(min(train_study_nums.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "157\n",
      "110\n",
      "185\n",
      "282\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "sample_size = min(train_study_nums.values())\n",
    "sampled_inds = {} # key -> sampled indices of len sample_size\n",
    "for key in train_study_nums:\n",
    "    n = train_study_nums[key]\n",
    "    sampled_inds[key] = random.sample(range(n), n) # sample_size)\n",
    "    print(len(sampled_inds[key]))\n",
    "# print(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.MTAB.386\n",
      "GSE13876\n",
      "GSE17260\n",
      "GSE18520\n",
      "GSE19829.GPL8300\n",
      "GSE26712\n",
      "GSE9891\n",
      "PMID17290060\n",
      "TCGA\n",
      "764.0\n",
      "131.0\n",
      "(1423, 2)\n",
      "(212, 2)\n"
     ]
    }
   ],
   "source": [
    "# create y label (y_train and y_test)\n",
    "count = 0\n",
    "y_train = np.asarray([])\n",
    "y_test = np.asarray([])\n",
    "for key in clean_pheno_datasets:\n",
    "    print(key)\n",
    "    dataset = clean_pheno_datasets[key]\n",
    "    index = np.argwhere(dataset[0] == pheno_name)[0][0]\n",
    "    labels = dataset[:,index][1:] == pheno_pos\n",
    "    assert len(labels) == len(clean_expr_datasets[key]) - 1\n",
    "    \n",
    "    if key not in test_studies: # count < num_train_studies:\n",
    "        labels = labels[sampled_inds[key]]\n",
    "        y_train = np.concatenate((y_train, labels.astype(float)))\n",
    "    else:\n",
    "        y_test = np.concatenate((y_test, labels.astype(float)))\n",
    "    count += 1\n",
    "\n",
    "y_train = np.expand_dims(y_train, 1)\n",
    "print(np.sum(y_train))\n",
    "y_train_expand = np.concatenate((1 - y_train, y_train), axis=1)\n",
    "y_test = np.expand_dims(y_test, 1)\n",
    "print(np.sum(y_test))\n",
    "y_test_expand = np.concatenate((1 - y_test, y_test), axis=1)\n",
    "print(y_train_expand.shape)\n",
    "print(y_test_expand.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E.MTAB.386\n",
      "GSE13876\n",
      "GSE17260\n",
      "GSE18520\n",
      "GSE19829.GPL8300\n",
      "GSE26712\n",
      "GSE9891\n",
      "PMID17290060\n",
      "TCGA\n",
      "(1423, 6063)\n",
      "(212, 6063)\n"
     ]
    }
   ],
   "source": [
    "# create x label (x_train and x_test)\n",
    "count = 0\n",
    "x_train = None\n",
    "x_test = None\n",
    "for key in clean_expr_datasets:\n",
    "    print(key)\n",
    "    dataset = clean_expr_datasets[key]\n",
    "    x_study = dataset[1:,1:].astype(float) # get rid of gene names and patient names\n",
    "\n",
    "    if key not in test_studies: # count < num_train_studies:\n",
    "        x_study = x_study[sampled_inds[key]]\n",
    "        if x_train is None:\n",
    "            x_train = x_study\n",
    "        else:\n",
    "            x_train = np.concatenate((x_train, x_study.astype(float)), axis=0)\n",
    "    else:\n",
    "        if x_test is None:\n",
    "            x_test = x_study\n",
    "        else:\n",
    "            x_test = np.concatenate((x_test, x_study.astype(float)), axis=0)\n",
    "    count += 1\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# fit only on training data\n",
    "scaler.fit(x_train)  \n",
    "x_train = scaler.transform(x_train)  \n",
    "# apply same transformation to test data\n",
    "x_test = scaler.transform(x_test) \n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train[128][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create study_id arrays\n",
    "study_ids = []\n",
    "dim = num_train_studies+1\n",
    "for i in range(dim):\n",
    "    study_id = np.zeros(dim)\n",
    "    study_id[i] = 1\n",
    "    study_ids.append(study_id)\n",
    "study_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7,)\n",
      "(129, 7)\n"
     ]
    }
   ],
   "source": [
    "study_index = 0\n",
    "\n",
    "for key in clean_expr_datasets:\n",
    "    dataset = clean_expr_datasets[key]  # row = gene, col = patient\n",
    "    study_id = study_ids[study_index]\n",
    "    print(study_id.shape)\n",
    "    num_samples = len(dataset[1:])\n",
    "    dup_study_ids = np.tile(study_id, (num_samples,1))\n",
    "    print(dup_study_ids.shape)\n",
    "    study_index += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1423, 7)\n",
      "(212, 7)\n"
     ]
    }
   ],
   "source": [
    "study_index = 0\n",
    "attr_train = None\n",
    "attr_test = None\n",
    "for key in clean_expr_datasets:\n",
    "    dataset = clean_expr_datasets[key]  # row = gene, col = patient\n",
    "    num_samples = len(dataset[1:])\n",
    "\n",
    "    if key not in test_studies: # study_index < num_train_studies: # \n",
    "        study_id = study_ids[study_index]\n",
    "        dup_study_ids = np.tile(study_id, (num_samples,1))\n",
    "        if attr_train is None:\n",
    "            attr_train = dup_study_ids\n",
    "        else:\n",
    "            attr_train = np.concatenate((attr_train, dup_study_ids), axis=0)  \n",
    "        study_index += 1\n",
    "    else:\n",
    "        study_id = study_ids[-1]\n",
    "        dup_study_ids = np.tile(study_id, (num_samples,1))\n",
    "        if attr_test is None:\n",
    "            attr_test = dup_study_ids\n",
    "        else:\n",
    "            attr_test = np.concatenate((attr_test, dup_study_ids), axis=0)\n",
    "\n",
    "print(attr_train.shape)\n",
    "print(attr_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.66805112 -0.03975443 -0.07168165 ... -0.5260554  -0.01737357\n",
      "  0.80217536]\n",
      "(6063,)\n",
      "[1. 0.]\n",
      "[1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(x_train[0].shape)\n",
    "print(y_train_expand[0])\n",
    "print(attr_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1138 285\n"
     ]
    }
   ],
   "source": [
    "t = x_train.shape[0]\n",
    "inds_shuffled = np.random.permutation(t)\n",
    "train_inds = inds_shuffled[:int(.8 * t)]\n",
    "valid_inds = inds_shuffled[int(.8 * t):]\n",
    "print(len(train_inds), len(valid_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212, 2)\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_expand.shape)\n",
    "print(y_test_expand[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = 'ovarian_6_studies_3_test_6063_features_all_samples_030921_standard.npz'\n",
    "np.savez(save_file, \n",
    "         x_train = x_train, \n",
    "         x_test = x_test, \n",
    "         y_train = y_train_expand, \n",
    "         y_test = y_test_expand, \n",
    "         attr_train = attr_train,\n",
    "         attr_test = attr_test,\n",
    "         train_inds = train_inds,\n",
    "         valid_inds = valid_inds,\n",
    "         num_train_studies = num_train_studies,\n",
    "         num_test_studies = num_studies - num_train_studies\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bio_infile = 'ovarian_6_studies_3_test_6063_features_all_samples_030921_standard.npz'\n",
    "bio_infile = \"ovarian_6_studies_3_test_6063_features_110_sample_size_020221_standard.npz\"\n",
    "data = np.load(bio_infile)\n",
    "\n",
    "x_train = data['x_train'][data['train_inds']]\n",
    "y_train = data['y_train'][data['train_inds']]\n",
    "attr_train = data['attr_train'][data['train_inds']]\n",
    "\n",
    "x_valid = data['x_train'][data['valid_inds']]\n",
    "y_valid = data['y_train'][data['valid_inds']]\n",
    "attr_valid = data['attr_train'][data['valid_inds']]\n",
    "\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n",
    "attr_test = data['attr_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528, 6063)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 6063)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 6063)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12491268952973736"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(x_test[:,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5696969696969697"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pos = np.sum(y_train[:,1])+np.sum(y_valid[:,1])\n",
    "total = y_train.shape[0] + y_valid.shape[0]\n",
    "num_pos/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n",
    "# num_samples = 1\n",
    "# nk = np.ones(k)*5000*num_samples\n",
    "# ind = np.repeat(0, nk[0])\n",
    "# for i in range(1, k-1):\n",
    "#     ind = np.concatenate((ind, np.repeat(i, nk[i])), axis=0)\n",
    "# attr_train = np.zeros((ind.size, ind.max()+1))\n",
    "# attr_train[np.arange(ind.size), ind] = 1\n",
    "# attr_test = np.concatenate( (np.ones((int(nk[-1]), 1)), np.zeros((int(nk[-1]), k-2))), axis=1)\n",
    "# print(attr_test.shape)\n",
    "# print(attr_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data_old(x, y, alpha=1.0):\n",
    "    '''\n",
    "    Returns mixed inputs, pairs of targets, and lambda\n",
    "    Code from https://github.com/facebookresearch/mixup-cifar10\n",
    "    '''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    index = np.random.permutation(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1. - lam) * x[index]\n",
    "    mixed_y = lam * y + (1. - lam) * y[index]\n",
    "    return mixed_x, mixed_y, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    '''\n",
    "    Returns mixed inputs, pairs of targets, and lambda\n",
    "    Code from https://github.com/facebookresearch/mixup-cifar10\n",
    "    '''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.\n",
    "\n",
    "    n = x.shape[0]\n",
    "    batch_size = n * n\n",
    "    mixed_x = np.zeros((batch_size, x.shape[1]))\n",
    "    mixed_y = np.zeros((batch_size, y.shape[1]))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            mixed_x[n*i + j] = lam * x[i] + (1. - lam) * x[j]\n",
    "            mixed_y[n*i + j] = lam * y[i] + (1. - lam) * y[j]\n",
    "\n",
    "    return mixed_x, mixed_y, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278784, 6063)\n",
      "(278784, 2)\n"
     ]
    }
   ],
   "source": [
    "# test_x, test_y, lam = mixup_data(x_train, y_train)\n",
    "# print(test_x.shape)\n",
    "# print(test_y.shape)\n",
    "# print(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data_with_a(x, y, a, alpha=1.0):\n",
    "    '''\n",
    "    Returns mixed inputs, pairs of targets, and lambda\n",
    "    Code from https://github.com/facebookresearch/mixup-cifar10\n",
    "    '''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.\n",
    "\n",
    "    batch_size = x.shape[0]\n",
    "    index = np.random.permutation(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1. - lam) * x[index]\n",
    "    mixed_y = lam * y + (1. - lam) * y[index]\n",
    "    mixed_a = lam * a + (1. - lam) * a[index]\n",
    "    return mixed_x, mixed_y, mixed_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_studies = len(attr_train[0])-1\n",
    "\n",
    "# create study_id arrays\n",
    "study_ids = []\n",
    "dim = num_train_studies+1\n",
    "for i in range(dim):\n",
    "    study_id = np.zeros(dim)\n",
    "    study_id[i] = 1\n",
    "    study_ids.append(study_id)\n",
    "study_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46464\n",
      "(528,)\n",
      "(85, 6063)\n",
      "(85, 2)\n",
      "0.27090891395475064\n",
      "(528,)\n",
      "(94, 6063)\n",
      "(94, 2)\n",
      "0.9455327187786406\n",
      "(528,)\n",
      "(87, 6063)\n",
      "(87, 2)\n",
      "0.8559001022772635\n",
      "(528,)\n",
      "(90, 6063)\n",
      "(90, 2)\n",
      "0.9093140924444223\n",
      "(528,)\n",
      "(88, 6063)\n",
      "(88, 2)\n",
      "0.22671366595716663\n",
      "(528,)\n",
      "(84, 6063)\n",
      "(84, 2)\n",
      "0.29823447340767095\n",
      "(46530, 6063)\n"
     ]
    }
   ],
   "source": [
    "n_squared = int(x_train.shape[0]/num_train_studies)**2\n",
    "total_n = num_train_studies * n_squared\n",
    "print(total_n)\n",
    "\n",
    "mixed_x_train = None\n",
    "mixed_y_train = None\n",
    "mixed_attr_train = None\n",
    "\n",
    "start_index = 0\n",
    "for i in range(num_train_studies):\n",
    "    study_mask = attr_train[:,i].astype(bool)\n",
    "    study_x = x_train[study_mask]\n",
    "    study_y = y_train[study_mask]\n",
    "    print(study_x.shape)\n",
    "    print(study_y.shape)\n",
    "    \n",
    "    study_mixed_x, study_mixed_y, lam = mixup_data(study_x, study_y)\n",
    "    print(lam)\n",
    "    \n",
    "    study_n = study_mixed_x.shape[0]\n",
    "    dup_study_ids = np.tile(study_ids[i], (study_n,1))\n",
    "    if mixed_x_train is None:\n",
    "        mixed_x_train = study_mixed_x\n",
    "        mixed_y_train = study_mixed_y\n",
    "        mixed_attr_train = dup_study_ids\n",
    "    else:\n",
    "        mixed_x_train = np.concatenate((mixed_x_train, study_mixed_x.astype(float)), axis=0)\n",
    "        mixed_y_train = np.concatenate((mixed_y_train, study_mixed_y.astype(float)))\n",
    "        mixed_attr_train = np.concatenate((mixed_attr_train, dup_study_ids), axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46530, 6063)\n",
      "(46530, 2)\n",
      "(46530, 7)\n",
      "[0.72909109 0.27090891]\n",
      "[1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(mixed_x_train.shape)\n",
    "print(mixed_y_train.shape)\n",
    "print(mixed_attr_train.shape)\n",
    "print(mixed_y_train[1])\n",
    "print(mixed_attr_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46530 132\n"
     ]
    }
   ],
   "source": [
    "t = mixed_x_train.shape[0]\n",
    "train_inds = np.random.permutation(t)\n",
    "v = x_valid.shape[0]\n",
    "valid_inds = np.random.permutation(v)\n",
    "print(len(train_inds), len(valid_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-0a263f34c161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msave_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ovarian_6_studies_3_test_6063_features_110_mixup_46530_samples_030921_standard.npz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m np.savez(save_file, \n\u001b[0;32m----> 6\u001b[0;31m          \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixed_x_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m          \u001b[0mx_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m          \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# THIS IS DIFFERENT, NEED TO UPDATE RVR, TRAIN AND VALID SEPARATE NOW BECAUSE MIXUP\n",
    "# RVR SHOULD SET x_train = x_train[train_inds] and x_valid = x_valid[valid_inds] instead of from x_train\n",
    "\n",
    "save_file = 'ovarian_6_studies_3_test_6063_features_110_mixup_46530_samples_030921_standard.npz'\n",
    "np.savez(save_file, \n",
    "         x_train = mixed_x_train[train_inds], \n",
    "         x_valid = x_valid[valid_inds],\n",
    "         x_test = x_test, \n",
    "         y_train = mixed_y_train[train_inds], \n",
    "         y_valid = y_valid[valid_inds],\n",
    "         y_test = y_test, \n",
    "         attr_train = mixed_attr_train[train_inds],\n",
    "         attr_valid = attr_valid[valid_inds],\n",
    "         attr_test = attr_test,\n",
    "#          train_inds = train_inds,\n",
    "#          valid_inds = valid_inds,\n",
    "         num_train_studies = num_train_studies,\n",
    "         num_test_studies = num_studies - num_train_studies\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_infile = \"ovarian_6_studies_3_test_6063_features_110_mixup_46530_samples_030921_standard.npz\"\n",
    "data = np.load(mixup_infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x_train',\n",
       " 'x_valid',\n",
       " 'x_test',\n",
       " 'y_train',\n",
       " 'y_valid',\n",
       " 'y_test',\n",
       " 'attr_train',\n",
       " 'attr_valid',\n",
       " 'attr_test',\n",
       " 'num_train_studies',\n",
       " 'num_test_studies']"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6179245283018868"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(data['x_train'].shape)\n",
    "# print(data['y_train'].shape)\n",
    "# print(data['attr_train'].shape)\n",
    "# print(data['x_valid'].shape)\n",
    "# print(data['y_valid'].shape)\n",
    "# print(data['attr_valid'].shape)\n",
    "num_pos = np.sum(data['y_test'][:,1]) #+np.sum(y_valid[:,1])\n",
    "total = data['y_test'].shape[0]\n",
    "num_pos/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.init_ops.GlorotUniform at 0x7fe54feaa310>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.glorot_uniform_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 6063)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape\n",
    "np.zeros(x.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = x_train.shape[0] # 64\n",
    "index = np.random.permutation(x_train.shape[0])\n",
    "x = x_train[index][:batch_size]\n",
    "y = y_train[index][:batch_size]\n",
    "a = attr_train[index][:batch_size]\n",
    "mixed_x, mixed_y, mixed_a = mixup_data_with_a(x, y, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.16610971, 0.        , 0.83389029,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(x_train)  \n",
    "x_train = scaler.transform(x_train)  \n",
    "# apply same transformation to test data\n",
    "x_test = scaler.transform(x_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_infile = \"ovarian_6_studies_3_test_6063_features_110_mixup_46530_samples_030921_standard.npz\"\n",
    "data = np.load(mixup_infile)\n",
    "x = data['x_train']\n",
    "y = data['y_train']\n",
    "y = np.round(y)\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=64, hidden_layer_sizes=(10, 10),\n",
       "              learning_rate_init=0.01, max_iter=500, random_state=1, tol=1e-08,\n",
       "              verbose=0)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier((10,10), max_iter=500, alpha=1e-4,\n",
    "                    solver='adam', verbose=0, tol=1e-8, random_state=1,\n",
    "                    learning_rate_init=.01, batch_size=64)\n",
    "clf.fit(x, y)\n",
    "# clf.fit(mixed_x, np.round(mixed_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6179245283018868\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# print(clf.score(mixed_x, mixed_y))\n",
    "plt.plot(clf.loss_curve_)\n",
    "# plt.plot(clf.validation_scores_)\n",
    "print(clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(target, pred, weights=None, eps=1e-8):\n",
    "    if weights == None:\n",
    "        weights = tf.ones_like(pred)\n",
    "    return -tf.squeeze(tf.multiply(weights, tf.multiply(target, tf.log(pred + eps)) + tf.multiply(1 - target, tf.log(1 - pred + eps))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.99999989e-09  1.84206807e+01]\n",
      " [-9.99999989e-09  1.84206807e+01]\n",
      " [ 1.84206807e+01 -9.99999989e-09]\n",
      " ...\n",
      " [ 1.84206807e+01 -9.99999989e-09]\n",
      " [-9.99999989e-09  1.84206807e+01]\n",
      " [ 1.84206807e+01 -9.99999989e-09]]\n"
     ]
    }
   ],
   "source": [
    "t = cross_entropy(y, np.zeros(y.shape))\n",
    "with tf.Session() as sess:\n",
    "    print(t.eval())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachelh/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-317-9444bc0c0e5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# MINI-BATCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_perm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmini_batch_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmini_batch_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mN_BATCH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m#         print(mlp.loss_curve_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmini_batch_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mN_BATCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_binarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_partial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_partial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    362\u001b[0m                       (not self.warm_start and not incremental))\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental, reset)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc'],\n\u001b[0m\u001b[1;32m    969\u001b[0m                                    \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'fc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[1;32m   2242\u001b[0m                           initial=initial, where=where)\n\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "import sklearn\n",
    "np.random.seed(1)\n",
    "\n",
    "\"\"\" Example based on sklearn's docs \"\"\"\n",
    "# mnist = fetch_mldata(\"MNIST original\")\n",
    "# rescale the data, use the traditional train/test split\n",
    "# X, y = mnist.data / 255., mnist.target\n",
    "X_train, X_test = x, x_test\n",
    "y_train, y_test = y, y_test\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=1, alpha=1e-4,\n",
    "                    solver='adam', verbose=0, tol=1e-8, random_state=1,\n",
    "                    learning_rate_init=.01)\n",
    "\n",
    "\"\"\" Home-made mini-batch learning\n",
    "    -> not to be used in out-of-core setting!\n",
    "\"\"\"\n",
    "N_TRAIN_SAMPLES = X_train.shape[0]\n",
    "N_EPOCHS = 200\n",
    "N_BATCH = 128\n",
    "N_CLASSES = np.unique(y_train)\n",
    "\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "# EPOCH\n",
    "epoch = 0\n",
    "while epoch < N_EPOCHS:\n",
    "#     print('epoch: ', epoch)\n",
    "    # SHUFFLING\n",
    "    random_perm = np.random.permutation(X_train.shape[0])\n",
    "    mini_batch_index = 0\n",
    "    num_batches = 0\n",
    "    while True:\n",
    "        # MINI-BATCH\n",
    "        indices = random_perm[mini_batch_index:mini_batch_index + N_BATCH]\n",
    "        mlp.partial_fit(X_train[indices], y_train[indices], classes=N_CLASSES)\n",
    "#         print(mlp.loss_curve_)\n",
    "        mini_batch_index += N_BATCH\n",
    "\n",
    "        num_batches += 1\n",
    "        if mini_batch_index >= N_TRAIN_SAMPLES:\n",
    "            break\n",
    "\n",
    "#     print(np.average(mlp.loss_curve_[-num_batches:]))\n",
    "    loss_train.append(np.average(mlp.loss_curve_[-num_batches:]))\n",
    "    # SCORE TRAIN\n",
    "    scores_train.append(mlp.score(X_train, y_train))\n",
    "\n",
    "    # SCORE TEST\n",
    "    scores_test.append(mlp.score(X_test, y_test))\n",
    "    \n",
    "#     train_pred = mlp.predict(X_train[indices])\n",
    "#     loss_train.append(sklearn.metrics.log_loss(y_train[indices], train_pred))\n",
    "    \n",
    "#     test_pred = mlp.predict(X_test)\n",
    "#     loss_test.append(sklearn.metrics.log_loss(y_test, test_pred))\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "\"\"\" Plot \"\"\"\n",
    "fig, ax = plt.subplots(2, sharex=True, sharey=True)\n",
    "ax[0].plot(scores_train)\n",
    "ax[0].set_title('Train Accuracy')\n",
    "ax[1].plot(scores_test)\n",
    "ax[1].set_title('Test Accuracy')\n",
    "# fig.suptitle(\"Accuracy over epochs\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, sharex=True, sharey=True)\n",
    "ax.plot(loss_train)\n",
    "ax.set_title('Train Loss')\n",
    "plt.show()\n",
    "\n",
    "print(loss_train[-1])\n",
    "\n",
    "print(scores_train[-1])\n",
    "print(scores_test[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe5625df0d0>]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Round:0' shape=(64, 2) dtype=float64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.round(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2)\n"
     ]
    }
   ],
   "source": [
    "pred = np.random.rand(y.shape[0], y.shape[1])\n",
    "print(pred.shape)\n",
    "target = y\n",
    "pred_class = tf.round(pred)\n",
    "acc = (1.0 - tf.reduce_mean(tf.cast(tf.equal(target, pred_class), tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.484375\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(acc.eval()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
