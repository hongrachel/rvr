{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# Convert pixel values from 0 to 255 to [0,1] range\n",
    "print(x_train.shape)\n",
    "x_train = np.reshape(x_train, (-1, 28*28))/255.\n",
    "print (x_train.shape)\n",
    "x_test = np.reshape(x_test, (-1, 28*28))/255.\n",
    "\n",
    "y_train = np.array([1 if y < 5 else 0 for y in y_train])\n",
    "y_test = np.array([1 if y < 5 else 0 for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0], 1)\n",
    "y_test = y_test.reshape(y_test.shape[0], 1)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train fraction\n",
    "frac = 0.8\n",
    "\n",
    "# shuffle dataset\n",
    "idx = np.random.randint(x_train.shape[0], size=len(x_train))\n",
    "X = x_train[idx]\n",
    "Y = y_train[idx]\n",
    "\n",
    "train_stop = int(len(X) * frac)\n",
    "\n",
    "X_ = X[:train_stop]\n",
    "Y_ = Y[:train_stop]\n",
    "\n",
    "X_v = X[train_stop:]\n",
    "Y_v = Y[train_stop:]\n",
    "Y_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda2/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] - weights1:0\n",
      "[[     0.]\n",
      " [-24591.]\n",
      " [     0.]\n",
      " [-24591.]\n",
      " [     0.]\n",
      " [-24591.]\n",
      " [     0.]\n",
      " [     0.]\n",
      " [     0.]\n",
      " [     0.]] - weightsOut:0\n",
      "weights1:0 - (784, 10) - [[-0.06428923  0.03716642 -0.01484323 ...  0.01288267  0.0195029\n",
      "   0.00454424]\n",
      " [ 0.03393454 -0.04771896 -0.00476133 ... -0.0210189  -0.01699767\n",
      "  -0.00412214]\n",
      " [-0.02495923  0.00566365  0.00206114 ...  0.03200849  0.0121543\n",
      "  -0.06042   ]\n",
      " ...\n",
      " [ 0.01520554 -0.01146381 -0.0569476  ...  0.01426436  0.02707958\n",
      "  -0.00961968]\n",
      " [-0.05049045 -0.00311015  0.03992454 ...  0.02669788 -0.03212445\n",
      "   0.00530215]\n",
      " [-0.01965043 -0.04982369  0.01306009 ...  0.00999025  0.0236108\n",
      "   0.00682935]]\n",
      "weightsOut:0 - (10, 1) - [[-82.13977 ]\n",
      " [-77.6313  ]\n",
      " [-84.69768 ]\n",
      " [-84.735916]\n",
      " [-86.78942 ]\n",
      " [-80.15687 ]\n",
      " [-90.145515]\n",
      " [-81.6857  ]\n",
      " [-83.00242 ]\n",
      " [-90.30879 ]]\n"
     ]
    }
   ],
   "source": [
    "# some hyperparams\n",
    "training_epochs = 100\n",
    "\n",
    "n_neurons_in_h1 = 10\n",
    "n_neurons_in_h2 = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_features = len(X_[0])\n",
    "labels_dim = 1\n",
    "\n",
    "#############################\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_features], name='input')\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='labels')\n",
    "# x = X_.astype(np.float32)\n",
    "# y = Y_.astype(np.float32)\n",
    "\n",
    "# TF Variables are our neural net parameter tensors, we initialize them to random (gaussian) values in\n",
    "# Layer1. Variables are allowed to be persistent across training epochs and updatable bt TF operations\n",
    "W1 = tf.Variable(tf.truncated_normal([n_features, n_neurons_in_h1], mean=0, stddev=1 / np.sqrt(n_features)),\n",
    "                 name='weights1')\n",
    "# b1 = tf.Variable(tf.truncated_normal([n_neurons_in_h1], mean=0, stddev=1 / np.sqrt(n_features)), name='biases1')\n",
    "\n",
    "# note the output tensor of the 1st layer is the activation applied to a\n",
    "# linear transform of the layer 1 parameter tensors\n",
    "# the matmul operation calculates the dot product between the tensors\n",
    "y1 = tf.sigmoid((tf.matmul(x, W1)), name='activationLayer1')\n",
    "\n",
    "# network parameters(weights and biases) are set and initialized (Layer2)\n",
    "# W2 = tf.Variable(tf.random_normal([n_neurons_in_h1, n_neurons_in_h2], mean=0, stddev=1),\n",
    "#                  name='weights2')\n",
    "# # b2 = tf.Variable(tf.random_normal([n_neurons_in_h2], mean=0, stddev=1), name='biases2')\n",
    "# # activation function(sigmoid)\n",
    "# y2 = tf.sigmoid((tf.matmul(y1, W2)), name='activationLayer2')\n",
    "\n",
    "# output layer weights and biases\n",
    "Wo = tf.Variable(tf.random_normal([n_neurons_in_h1, labels_dim], mean=0, stddev=1 ),\n",
    "                 name='weightsOut')\n",
    "# bo = tf.Variable(tf.random_normal([labels_dim], mean=0, stddev=1), name='biasesOut')\n",
    "\n",
    "# the sigmoid (binary softmax) activation is absorbed into TF's sigmoid_cross_entropy_with_logits loss\n",
    "logits = (tf.matmul(y1, Wo))\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "\n",
    "train = optimizer.apply_gradients(grads_and_vars)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1):\n",
    "    sess.run(train, feed_dict={x: X_, y: Y_})\n",
    "    for gv in grads_and_vars:\n",
    "        if gv[0] is not None:\n",
    "            print(str(sess.run(gv[0], feed_dict={x: X_, y: Y_})) + \" - \" + str(gv[1].name))\n",
    "        #print(step, sess.run(W), sess.run(b))\n",
    "    weights = [v for v in tf.trainable_variables() if v.name == \"weights1:0\" or v.name == \"weightsOut:0\"]\n",
    "    for weight in weights:\n",
    "        val = sess.run(weight)\n",
    "        print(weight.name + \" - \" + str(val.shape) + \" - \" + str(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13023472] - Wvar_1:0\n",
      "[0.40048462] - Bvar_1:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name = \"Wvar\")\n",
    "b = tf.Variable(tf.zeros([1]), name = \"Bvar\")\n",
    "y = W * x_data + b\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "\n",
    "train = optimizer.apply_gradients(grads_and_vars)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1):\n",
    "    sess.run(train)\n",
    "    for gv in grads_and_vars:\n",
    "        if gv[0] is not None:\n",
    "            print(str(sess.run(gv[0])) + \" - \" + str((gv[1].name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
